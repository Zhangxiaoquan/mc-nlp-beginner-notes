接11-10

### cost function

+ loss function（$L(\hat{y}, y)$）是用于单一训练样本而言的损失
+ cost function $J(w,b) = \frac{1}{m}\sum_m^{i=1}L(\hat{y}^{(i)}, y^{(i)})$是**基于参数**针对于所有训练样本的平均损失，也就是在全量的训练样本中的效果

+ 所以此时就是我们寻找$w,b$如何最小化$J(w,b)$


# 梯度下降

+ cost function 是一个凸函数（convex function），并且有唯一的最小点
  + 这也是为什么我们不采用方差，而是采用了交叉熵作为loss function

+ 想法从最初点每次找一个最陡的方向走

+ 学习率，每次走都的方向走的步长
+ 导数，在这个点的斜率

## 导数

+ 斜率即导数
$$\frac{df(a)}{da}$$
+ $d$表示右移一个不可度量的小的值，$f(a)$所增加的值
+ 一个函数的每个点斜率不同