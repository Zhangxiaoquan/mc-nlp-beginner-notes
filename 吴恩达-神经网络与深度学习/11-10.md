# logistic Regression

## 解决问题
+ 是监督学习中解决分类问题
+ 输入是一个$X$是一个$1 \times n$的矩阵
+ 参数$w$也是一个$1 \times n$的矩阵, 参数b为一个变量
+ 再将线性函数$w^t \times x + b$ 模拟到一个0-1的概率上，用sigmoid函数，从而通过概率解决分布的问题

## 训练参数$w,b$

### loss function(损失函数)
+ 用于评估通过逻辑模型得到的预测值和对应的真实结果之间的差异
+ 方差可以反映差异，不过由于其拥有大量的局部最优解，所以梯度下降效果不好，一般不用
$$ L(\hat{y},y) = -y\log{\hat{y}} + (1-y)\log{(1-\hat{y})}$$
+ **这个损失函数特别像信息熵的表达方式**
+ 交叉熵

### [交叉熵](http://yanbc.info/2019/02/24/cross-entropy/)

+ 信息熵是对于一个概率分布描述他的编码长度，但是假设我们不知道他真实的概率分布$P$, 估算了一个概率分布$Q$
+ 此时机油了表示两个概率分布的平均编码长度的交叉熵
$$-1 \times P \times \log{Q}$$
+ 这个公式表示$Q$的概率分布越接近$P$, 那么越小（可以证明）
+ **完美切合损失函数**

### cost function