# computing graph

+ 就是讲计算表达式按照计算顺序得到结果的过程，一步步的通过图的节点进行表示，图节点表示一个输入或者中间结果。
+ 有向图
+ 从输入到输出

## [积分的链式法则（chaining rule）](https://zh.wikipedia.org/wiki/%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99)

+ 是复核函数倒数的一个法则
+ 如果对于computing graph逆向求导会用到

## logistic regression 的梯度下降

+ 首先知道cost function，因为是梯度下降所以，对于cost function进行求导，对于最外层进行求导，交叉熵求导
+ 在根据链式法则，得到对倒数第二层变量的导数，交叉熵乘以sigmoid函数的导数
+ 同样根据链式法则对于输入层参数变量求导
+ 此时对于一个样本输入对应的$w,b$输入参数就可以基于求导的结果进行梯度下降了
+ 如果有大量的样本输入的话，cost function是一个loss function的平均，所以梯度也就变成了多个样本的求导结果的平均

### 小技巧

+ 因为在梯度下降的过程中，首先可能对大量的对于下降过程进行循环，其次对于每个训练数据，对于每个参数都进行训练，可能会显示的出现很多的for循环
+ for循环效率很低，尽可能的采用矩阵运算而不是for循环来求解