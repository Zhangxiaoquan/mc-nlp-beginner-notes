# 信息的度量和作用
## 信息熵
+ 把一个信息搞清楚需要的信息量
+ 所以信息熵越高，不确定性越高
+ nlp也是一个减少信息不确定性的过程
+ 当对于一个不确定性的问题需要解决,不是不停的修改算法,或者加上认为的假设,**信息加入**是十分关键,因为新的信息是可以减少不确定性

## 条件熵
+ 条件熵是对于$X$的信息熵在加入信息$Y$的情况的信息熵
+ 只要$Y$与$X$相关，那么条件熵一定小于信息熵，也就是表明，信息加入会减少不确定性
+ 相关信息（可以理解为上下文）引入更多会大大减少不确定性，但是会增加复杂度，例如N元模型，N越大越精确，训练成本越高

## 互信息

+ 信息熵和条件熵之间的差值
+ 也就是引入$Y$信息后能消除的不确定性

## 相对熵

+ 用于反应两种信号函数的差异性
+ 后来应用于概率分布的差异性
+ 所以在nlp中基本用于相同词汇在不同文本中的概率分布

利用信息熵，条件熵，相对熵能精确的衡量一个语言模型的准确度