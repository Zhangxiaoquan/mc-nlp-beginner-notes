# 隐马尔科夫模型

## 通讯模型
信息+上下文 -> $s_1,s_2...$ -> 编码 -> 信息通道传递 -> $o_1, o_2 ...$ -> 解码 -> 信息

几乎所有的自然语言处理的问题都可以理解成为通讯模型的**解码**问题。

所以如果对于一个翻译问题，假设我们得到的信号是$o_1,o_2...$，这个时候希望得到的他的原本信息$s_1,s_2...$，此时就是求$o_1,o_2...$的情况下出现$s_1,s_2...$的最高概率。

## [贝叶斯公式](https://www.zhihu.com/question/19725590)
$$P(A|B) = P(A)\frac{P(B|A)}{P(B)}$$
+ A是求解的问题
+ B是已知条件
核心概念：在有限系的信息预测概率（逆推导）


### 先验概率$P(A)$
+ 是一个主观概率
+ 在不知道已知条件B

### 可能函数$\frac{P(B|A)}{P(B)}$
+ 已知条件B带来的调整

### 后验概率
+ 也就是在发生B事件以后对于A事件概率的一个调整后的概率

## Hidden Markov Model
通过通讯问题这个概率，我们结合贝叶斯公式我们可以得到
$$P(s_1,s_2...|o_1,o_2...)=P(s_1,s_2...)\frac{P(o_1,o_2...|s_1,s_2...)}{P(o_1,o_2....)}$$

此时$P(s_1,s_2...)P(o_1,o_2...|s_1,s_2...)$是一个待求概率，这个可以用Hidden Markov Model估计

### Markov Model

核心想法：每个状态的概率只与前一个状态有关

### Hidden Markov Model
每个状态不可见，但是可以看见的输出$o_t$是唯一关联$s_t$,并且$s_t$是一个马尔科夫链

成功应用
+ 语音识别，o是语音输入，s是翻译语句
+ 机器翻译，o是输入语句，s翻译语句

### 训练求解模型
如果有足够的有标数据，通过概率就可以算出来转移概率$P(s_t|s_t-1)$和生成概率$P(o_t|s_t)$
鲍姆韦尔奇算法可以利用无标数据训练模型参数

#### [鲍姆韦尔奇算法（EM算法）](http://www.woshipm.com/ai/3968240.html)
+ 典型的非监督学习
+ 极大似然 = 最大可能 = 知道样本结果，反推条件
+ E 随便猜一个，基于猜测进行划分
+ M 根据划分结果再重新调参数，在进行E
#### [维比特算法](https://www.zhihu.com/question/20136144)
+ 删除不可能是答案的路径以降低复杂度